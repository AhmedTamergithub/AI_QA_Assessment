import sys
import json
import os
from typing import Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
from google import genai
from dotenv import load_dotenv
# FastMCP Import
from mcp.server.fastmcp import FastMCP
mcp = FastMCP("evaluation-mcp-server")
# Load environment variables
load_dotenv()

# Configure Gemini API
client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# Initialize embedding model for similarity calculation
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')


def evaluate_llm_responses(response: str, ground_truth: str, threshold: float = 0.7) -> Dict[str, Any]:
    """
    Evaluate the similarity between an LLM-generated response and ground truth using cosine similarity.
    
    This tool measures semantic similarity using embeddings to determine if the generated summary
    preserves the meaning of the original source text.
    
    Args:
        response (str): The summary generated by the summarization agent
        ground_truth (str): The original text extracted from the PDF
        threshold (float): Minimum similarity score to pass (default: 0.7)
        
    Returns:
        Dict[str, Any]: Evaluation results containing:
            - similarity_score (float): Cosine similarity score (0-1)
            - verdict (str): "PASS" if similarity >= threshold, else "FAIL"
            - threshold (float): The threshold used for evaluation
            - message (str): Human-readable explanation
            
    Raises:
        ValueError: If response or ground_truth is empty
    """
    try:
        # Validate inputs
        if not response or not response.strip():
            raise ValueError("Response text is empty or contains only whitespace")
        if not ground_truth or not ground_truth.strip():
            raise ValueError("Ground truth text is empty or contains only whitespace")
        
        # Generate embeddings for both texts
        response_embedding = embedding_model.encode([response])
        ground_truth_embedding = embedding_model.encode([ground_truth])
        
        # Calculate cosine similarity
        similarity_score = cosine_similarity(response_embedding, ground_truth_embedding)[0][0]
        
        # Determine verdict
        verdict = "PASS" if similarity_score >= threshold else "FAIL"
        
        # Create evaluation result
        result = {
            "similarity_score": float(similarity_score),
            "verdict": verdict,
            "threshold": threshold,
            "message": f"Semantic similarity: {similarity_score:.4f} ({'above' if similarity_score >= threshold else 'below'} threshold of {threshold})"
        }
        
        return result
    
    except Exception as e:
        raise Exception(f"Error in similarity evaluation: {str(e)}")


def hallucination_checker(response: str, ground_truth: str, model_name: str = "gemini-3-flash-preview") -> Dict[str, Any]:
    """
    Check for hallucinations in the LLM-generated response using an LLM-as-a-judge approach.
    
    This tool uses a separate LLM (Gemini 3 Flash) to verify that all information in the response
    can be found or inferred from the ground truth, flagging any fabricated or hallucinated content.
    
    Args:
        response (str): The summary generated by the summarization agent
        ground_truth (str): The original text extracted from the PDF
        model_name (str): The Gemini model to use as judge (default: "gemini-1.5-flash")
        
    Returns:
        Dict[str, Any]: Evaluation results containing:
            - has_hallucination (bool): True if hallucinations detected
            - verdict (str): "PASS" if no hallucinations, else "FAIL"
            - confidence (str): Judge's confidence level (high/medium/low)
            - hallucinated_claims (list): List of specific hallucinated statements (if any)
            - explanation (str): Detailed reasoning from the judge
            - judge_model (str): The model used for judgment
            
    Raises:
        ValueError: If response or ground_truth is empty
        Exception: For API errors or other issues
    """
    try:
        # Validate inputs
        if not response or not response.strip():
            raise ValueError("Response text is empty or contains only whitespace")
        if not ground_truth or not ground_truth.strip():
            raise ValueError("Ground truth text is empty or contains only whitespace")
        
        # Create the judge prompt
        judge_prompt = f"""You are an expert fact-checker evaluating whether a summary contains hallucinations.

**Task**: Determine if the SUMMARY contains any information that cannot be found or reasonably inferred from the SOURCE TEXT.

**SOURCE TEXT**:
{ground_truth}

**SUMMARY**:
{response}

**Instructions**:
1. Carefully read both the source text and the summary
2. Identify any claims, facts, or details in the summary that are NOT present in the source
3. Distinguish between:
   - Valid abstractions/generalizations (acceptable)
   - Information that can be reasonably inferred (acceptable)
   - Fabricated or added information (hallucination)

**Response Format** (JSON):
{{
  "has_hallucination": true/false,
  "confidence": "high/medium/low",
  "hallucinated_claims": ["claim 1", "claim 2", ...],
  "explanation": "Detailed reasoning for your decision"
}}

Respond ONLY with valid JSON, no additional text."""

        # Call Gemini API for judgment
        judge_response = client.models.generate_content(
            model=model_name,
            contents=judge_prompt
        )
        
        # Parse the JSON response
        response_text = judge_response.text.strip()
        
        # Extract JSON from markdown code blocks if present
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        judgment = json.loads(response_text)
        
        # Validate judgment structure
        has_hallucination = judgment.get("has_hallucination", False)
        
        # Determine verdict
        verdict = "FAIL" if has_hallucination else "PASS"
        
        # Create evaluation result
        result = {
            "has_hallucination": has_hallucination,
            "verdict": verdict,
            "confidence": judgment.get("confidence", "unknown"),
            "hallucinated_claims": judgment.get("hallucinated_claims", []),
            "explanation": judgment.get("explanation", "No explanation provided"),
            "judge_model": model_name
        }
        
        return result
    
    except json.JSONDecodeError as e:
        raise Exception(f"Failed to parse judge response as JSON: {str(e)}")
    except Exception as e:
        raise Exception(f"Error in hallucination check: {str(e)}")


def load_summarization_data(summary_file_path: Optional[str] = None, 
                            raw_data_file_path: Optional[str] = None) -> Dict[str, str]:
    """
    Helper function to load summarization data from JSON files.
    
    Args:
        summary_file_path (Optional[str]): Path to the summary JSON file. 
            Defaults to summarization_agent/output/summarize_after_chunks.json
        raw_data_file_path (Optional[str]): Path to the raw data JSON file.
            Defaults to summarization_agent/output/raw_extracted_data.json
            
    Returns:
        Dict[str, str]: Dictionary containing:
            - combined_summary (str): The generated summary
            - extracted_text (str): The original source text
            
    Raises:
        FileNotFoundError: If the files don't exist
        KeyError: If expected keys are missing from the JSON
    """
    try:
        # Set default paths if not provided
        if summary_file_path is None:
            summary_file_path = os.path.join(
                os.path.dirname(__file__), 
                "..", "summarization_agent", "output", "summarize_after_chunks.json"
            )
        
        if raw_data_file_path is None:
            raw_data_file_path = os.path.join(
                os.path.dirname(__file__), 
                "..", "summarization_agent", "output", "raw_extracted_data.json"
            )
        
        # Load summary data
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
        
        # Load raw extracted data
        with open(raw_data_file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        return {
            "combined_summary": summary_data["combined_summary"],
            "extracted_text": raw_data["extracted_text"]
        }
    
    except FileNotFoundError as e:
        raise FileNotFoundError(f"Data file not found: {str(e)}")
    except KeyError as e:
        raise KeyError(f"Expected key missing from JSON: {str(e)}")
    except Exception as e:
        raise Exception(f"Error loading summarization data: {str(e)}")

@mcp.tool()
def evaluate_summarization_agent(summary_file_path: Optional[str] = None,
                                 raw_data_file_path: Optional[str] = None,
                                 similarity_threshold: float = 0.7) -> str:
    """
    Complete evaluation of summarization agent output using both similarity and hallucination checks.
    
    Args:
        summary_file_path (Optional[str]): Path to the summary JSON file
        raw_data_file_path (Optional[str]): Path to the raw data JSON file
        similarity_threshold (float): Minimum similarity score to pass (default: 0.7)
        
    Returns:
        JSON string containing evaluation results:
            - overall_verdict (str): "PASS" if both checks pass, else "FAIL"
            - similarity_evaluation (Dict): Results from similarity check
            - hallucination_evaluation (Dict): Results from hallucination check
            - recommendation (str): Action recommendation based on results
    """
    try:
        # Load data
        data = load_summarization_data(summary_file_path, raw_data_file_path)
        
        # Run similarity evaluation
        similarity_result = evaluate_llm_responses(
            response=data["combined_summary"],
            ground_truth=data["extracted_text"],
            threshold=similarity_threshold
        )
        
        # Run hallucination check
        hallucination_result = hallucination_checker(
            response=data["combined_summary"],
            ground_truth=data["extracted_text"]
        )
        
        # Determine overall verdict
        overall_pass = (
            similarity_result["verdict"] == "PASS" and 
            hallucination_result["verdict"] == "PASS"
        )
        overall_verdict = "PASS" if overall_pass else "FAIL"
        
        # Generate recommendation
        if overall_pass:
            recommendation = "Summary is acceptable. High semantic similarity and no hallucinations detected."
        elif similarity_result["verdict"] == "FAIL" and hallucination_result["verdict"] == "FAIL":
            recommendation = "CRITICAL: Low similarity AND hallucinations detected. Regenerate summary with stricter instructions."
        elif similarity_result["verdict"] == "FAIL":
            recommendation = "Low semantic similarity. Summary may be too abstract or missing key information. Consider regenerating."
        else:  # hallucination_result["verdict"] == "FAIL"
            recommendation = "Hallucinations detected. Review and regenerate summary to ensure factual accuracy."
        
        # Compile complete evaluation
        result = {
            "overall_verdict": overall_verdict,
            "similarity_evaluation": similarity_result,
            "hallucination_evaluation": hallucination_result,
            "recommendation": recommendation,
            "evaluated_summary": data["combined_summary"][:200] + "..." if len(data["combined_summary"]) > 200 else data["combined_summary"]
        }
        
        return json.dumps(result, indent=2)
    
    except Exception as e:
        return json.dumps({"error": f"Error in complete evaluation: {str(e)}"})


if __name__ == "__main__":
    print("Launching Evaluation MCP Server via stdio...", file=sys.stderr)
    mcp.run()

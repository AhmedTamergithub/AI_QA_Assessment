In the context of an automated **Evaluation Agent**, "ground truth" refers to the target or reference data that the AIâ€™s response is measured against. In Task 1 and Task 2 of your assignment, the ground truth is obtained differently depending on which tool you are testing:

### 1. Where does the Ground Truth come from?

Depending on the scenario, you will provide the ground truth in one of two ways:

* **For the Summarization Tool:** The "ground truth" is typically the **original full text** or the **source document**. The Evaluation Agent compares the summary against this source to ensure no important info was missed (Similarity) and no fake info was added (Hallucination).
* **For the API Fetching Tool:** The ground truth is the **raw JSON response** directly from the public API (Weather or Exchange Rate). You compare the LLM's natural language explanation against this raw data to ensure the numbers match.
* **For Testing/QA (Task 3):** You might hardcode a "Known Good" answer in your Playwright tests to see if the system correctly identifies it as a match.

### 2. Implementation: How to "get" it for the tools

You don't necessarily "fetch" ground truth from a third party; you **pass it** to the evaluation tools as an argument.

#### **A. For `evaluate_llm_responses` (Similarity Tool)**

This tool needs two inputs to do its math:

1. **`response`**: The summary generated by your `summarize_text` tool.
2. **`ground_truth`**: The original text extracted by `extract_pdf_text`.

* *Logic:* If the Cosine Similarity is high, the summary kept the meaning of the source.

#### **B. For `hallucination_checker**`

This tool uses an LLM-as-a-judge.

1. **`response`**: The summary.
2. **`ground_truth`**: The original source text.

* *Logic:* The LLM reads both and flags any information in the "response" that cannot be found in the "ground truth".

### 3. Visualizing the Data Flow

Here is how the ground truth moves through your architecture:

1. **Root Agent** triggers the **Summarization Agent**.
2. Summarization Agent returns a **Summary**.
3. **Root Agent** then calls the **Evaluation Agent**, sending it **BOTH** the original text (Ground Truth) and the new Summary (Response).
4. Evaluation Agent runs the tools and returns the scores.

### Summary for your Implementation:

In your code, when you call the `hallucination_checker` or `evaluate_llm_responses` tools, you must ensure your **Root Agent** has saved the original text from the earlier step so it can provide it as the `ground_truth` argument.








#### EVALUATION IN SUB AGENTS OR SEQUENTIAL AFTER 2 MAIN AGENTS?
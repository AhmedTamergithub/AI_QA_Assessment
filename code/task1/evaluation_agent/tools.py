import json
import os
from typing import Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np
from google import genai
from dotenv import load_dotenv
import requests
from google.adk.tools.tool_context import ToolContext
import re
from api_fetching_agent.tools import fetch_weather, fetch_exchange_rate

# Load environment variables

load_dotenv()

# Configure Gemini API
client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# Initialize embedding model for similarity calculation
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')


def evaluate_llm_responses(response: str, ground_truth: str, threshold: float = 0.7) -> Dict[str, Any]:
    """
    Evaluate the similarity between an LLM-generated response and ground truth using cosine similarity.
    
    This tool measures semantic similarity using embeddings to determine if the generated summary
    preserves the meaning of the original source text.
    
    Args:
        response (str): The summary generated by the summarization agent
        ground_truth (str): The original text extracted from the PDF
        threshold (float): Minimum similarity score to pass (default: 0.7)
        
    Returns:
        Dict[str, Any]: Evaluation results containing:
            - similarity_score (float): Cosine similarity score (0-1)
            - verdict (str): "PASS" if similarity >= threshold, else "FAIL"
            - threshold (float): The threshold used for evaluation
            - message (str): Human-readable explanation
            
    Raises:
        ValueError: If response or ground_truth is empty
    """
    try:
        # Validate inputs
        if not response or not response.strip():
            raise ValueError("Response text is empty or contains only whitespace")
        if not ground_truth or not ground_truth.strip():
            raise ValueError("Ground truth text is empty or contains only whitespace")
        
        # Generate embeddings for both texts
        response_embedding = embedding_model.encode([response])
        ground_truth_embedding = embedding_model.encode([ground_truth])
        
        # Calculate cosine similarity
        similarity_score = cosine_similarity(response_embedding, ground_truth_embedding)[0][0]
        
        # Determine verdict
        verdict = "PASS" if similarity_score >= threshold else "FAIL"
        
        # Create evaluation result
        result = {
            "similarity_score": float(similarity_score),
            "verdict": verdict,
            "threshold": threshold,
            "message": f"Semantic similarity: {similarity_score:.4f} ({'above' if similarity_score >= threshold else 'below'} threshold of {threshold})"
        }
        
        return result
    
    except Exception as e:
        raise Exception(f"Error in similarity evaluation: {str(e)}")


def hallucination_checker(response: str, ground_truth: str, model_name: str = "gemini-3-flash-preview") -> Dict[str, Any]:
    """
    Check for hallucinations in the LLM-generated response using an LLM-as-a-judge approach.
    
    This tool uses a separate LLM (Gemini 3 Flash) to verify that all information in the response
    can be found or inferred from the ground truth, flagging any fabricated or hallucinated content.
    
    Args:
        response (str): The summary generated by the summarization agent
        ground_truth (str): The original text extracted from the PDF
        model_name (str): The Gemini model to use as judge (default: "gemini-1.5-flash")
        
    Returns:
        Dict[str, Any]: Evaluation results containing:
            - has_hallucination (bool): True if hallucinations detected
            - verdict (str): "PASS" if no hallucinations, else "FAIL"
            - confidence (str): Judge's confidence level (high/medium/low)
            - hallucinated_claims (list): List of specific hallucinated statements (if any)
            - explanation (str): Detailed reasoning from the judge
            - judge_model (str): The model used for judgment
            
    Raises:
        ValueError: If response or ground_truth is empty
        Exception: For API errors or other issues
    """
    try:
        # Validate inputs
        if not response or not response.strip():
            raise ValueError("Response text is empty or contains only whitespace")
        if not ground_truth or not ground_truth.strip():
            raise ValueError("Ground truth text is empty or contains only whitespace")
        
        # Create the judge prompt
        judge_prompt = f"""You are an expert fact-checker evaluating whether a summary contains hallucinations.

**Task**: Determine if the SUMMARY contains any information that cannot be found or reasonably inferred from the SOURCE TEXT.

**SOURCE TEXT**:
{ground_truth}

**SUMMARY**:
{response}

**Instructions**:
1. Carefully read both the source text and the summary
2. Identify any claims, facts, or details in the summary that are NOT present in the source
3. Distinguish between:
   - Valid abstractions/generalizations (acceptable)
   - Information that can be reasonably inferred (acceptable)
   - Fabricated or added information (hallucination)

**Response Format** (JSON):
{{
  "has_hallucination": true/false,
  "confidence": "high/medium/low",
  "hallucinated_claims": ["claim 1", "claim 2", ...],
  "explanation": "Detailed reasoning for your decision"
}}

Respond ONLY with valid JSON, no additional text."""

        # Call Gemini API for judgment
        judge_response = client.models.generate_content(
            model=model_name,
            contents=judge_prompt
        )
        
        # Parse the JSON response
        response_text = judge_response.text.strip()
        
        # Extract JSON from markdown code blocks if present
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        judgment = json.loads(response_text)
        
        # Validate judgment structure
        has_hallucination = judgment.get("has_hallucination", False)
        
        # Determine verdict
        verdict = "FAIL" if has_hallucination else "PASS"
        
        # Create evaluation result
        result = {
            "has_hallucination": has_hallucination,
            "verdict": verdict,
            "confidence": judgment.get("confidence", "unknown"),
            "hallucinated_claims": judgment.get("hallucinated_claims", []),
            "explanation": judgment.get("explanation", "No explanation provided"),
            "judge_model": model_name
        }
        
        return result
    
    except json.JSONDecodeError as e:
        raise Exception(f"Failed to parse judge response as JSON: {str(e)}")
    except Exception as e:
        raise Exception(f"Error in hallucination check: {str(e)}")


def load_summarization_data(summary_file_path: Optional[str] = None, 
                            raw_data_file_path: Optional[str] = None) -> Dict[str, str]:
    """
    Helper function to load summarization data from JSON files.
    
    Args:
        summary_file_path (Optional[str]): Path to the summary JSON file. 
            Defaults to summarization_agent/output/summarize_after_chunks.json
        raw_data_file_path (Optional[str]): Path to the raw data JSON file.
            Defaults to summarization_agent/output/raw_extracted_data.json
            
    Returns:
        Dict[str, str]: Dictionary containing:
            - combined_summary (str): The generated summary
            - extracted_text (str): The original source text
            
    Raises:
        FileNotFoundError: If the files don't exist
        KeyError: If expected keys are missing from the JSON
    """
    try:
        # Set default paths if not provided
        if summary_file_path is None:
            summary_file_path = os.path.join(
                os.path.dirname(__file__), 
                "..", "summarization_agent", "output", "summarize_after_chunks.json"
            )
        
        if raw_data_file_path is None:
            raw_data_file_path = os.path.join(
                os.path.dirname(__file__), 
                "..", "summarization_agent", "output", "raw_extracted_data.json"
            )
        
        # Load summary data
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
        
        # Load raw extracted data
        with open(raw_data_file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        return {
            "combined_summary": summary_data["combined_summary"],
            "extracted_text": raw_data["extracted_text"]
        }
    
    except FileNotFoundError as e:
        raise FileNotFoundError(f"Data file not found: {str(e)}")
    except KeyError as e:
        raise KeyError(f"Expected key missing from JSON: {str(e)}")
    except Exception as e:
        raise Exception(f"Error loading summarization data: {str(e)}")


def evaluate_summarization_agent(summary_file_path: Optional[str] = None,
                                 raw_data_file_path: Optional[str] = None,
                                 similarity_threshold: float = 0.7) -> Dict[str, Any]:
    """
    Complete evaluation of summarization agent output using both similarity and hallucination checks.
    
    Args:
        summary_file_path (Optional[str]): Path to the summary JSON file
        raw_data_file_path (Optional[str]): Path to the raw data JSON file
        similarity_threshold (float): Minimum similarity score to pass (default: 0.7)
        
    Returns:
        Dict[str, Any]: Complete evaluation results containing:
            - overall_verdict (str): "PASS" if both checks pass, else "FAIL"
            - similarity_evaluation (Dict): Results from similarity check
            - hallucination_evaluation (Dict): Results from hallucination check
            - recommendation (str): Action recommendation based on results
            
    Raises:
        FileNotFoundError: If data files don't exist
        Exception: For evaluation errors
    """
    try:
        # Load data
        data = load_summarization_data(summary_file_path, raw_data_file_path)
        
        # Run similarity evaluation
        similarity_result = evaluate_llm_responses(
            response=data["combined_summary"],
            ground_truth=data["extracted_text"],
            threshold=similarity_threshold
        )
        
        # Run hallucination check
        hallucination_result = hallucination_checker(
            response=data["combined_summary"],
            ground_truth=data["extracted_text"]
        )
        
        # Determine overall verdict
        overall_pass = (
            similarity_result["verdict"] == "PASS" and 
            hallucination_result["verdict"] == "PASS"
        )
        overall_verdict = "PASS" if overall_pass else "FAIL"
        
        # Generate recommendation
        if overall_pass:
            recommendation = "Summary is acceptable. High semantic similarity and no hallucinations detected."
        elif similarity_result["verdict"] == "FAIL" and hallucination_result["verdict"] == "FAIL":
            recommendation = "CRITICAL: Low similarity AND hallucinations detected. Regenerate summary with stricter instructions."
        elif similarity_result["verdict"] == "FAIL":
            recommendation = "Low semantic similarity. Summary may be too abstract or missing key information. Consider regenerating."
        else:  # hallucination_result["verdict"] == "FAIL"
            recommendation = "Hallucinations detected. Review and regenerate summary to ensure factual accuracy."
        
        # Compile complete evaluation
        result = {
            "overall_verdict": overall_verdict,
            "similarity_evaluation": similarity_result,
            "hallucination_evaluation": hallucination_result,
            "recommendation": recommendation,
            "evaluated_summary": data["combined_summary"][:200] + "..." if len(data["combined_summary"]) > 200 else data["combined_summary"]
        }
        
        return result
    
    except Exception as e:
        raise Exception(f"Error in complete evaluation: {str(e)}")




def evaluate_api_fetching_agent(tool_context: ToolContext):
    """
    Evaluate the API fetching agent by comparing its output with real API calls.
    """
    raw_data = tool_context.state.get("api_fetching_results")
    
    if not raw_data:
        return "Error: No api_fetching_results found in state."

    # Parse JSON if it's a string (with regex for markdown)
    if isinstance(raw_data, str):
        # Try to find JSON in markdown blocks first
        match = re.search(r"```json\s*(\{.*?\})\s*```", raw_data, re.DOTALL)
        if match:
            raw_data = match.group(1)
        
        try:
            data = json.loads(raw_data)
        except json.JSONDecodeError:
            # If it's not valid JSON, it might be a conversational response
            return {
                "status": "skipped",
                "message": "Agent output is not in JSON format. This is expected for conversational responses. Evaluation only runs on final JSON output (when user says 'finish').",
                "agent_output_preview": raw_data[:100] + "..." if len(raw_data) > 100 else raw_data
            }
    else:
        data = raw_data

    results = {}

    # 1. Evaluate Weather
    if data.get("weather") and isinstance(data["weather"], dict):
        weather_info = data["weather"]
        city = weather_info.get("city", "").split(",")[0] # Extract city name
        agent_temp = weather_info.get("temperature")
        
        if city and agent_temp is not None:
            try:
                # Call ground truth
                gt_weather = fetch_weather(city)
                gt_temp = gt_weather["weather"]["temperature"]
                
                results["weather_match"] = abs(agent_temp - gt_temp) < 2.0
                results["weather_details"] = {"agent": agent_temp, "ground_truth": gt_temp}
            except Exception as e:
                results["weather_evaluation_error"] = str(e)

    # 2. Evaluate Exchange Rate
    if data.get("exchange_rate") and isinstance(data["exchange_rate"], dict):
        rate_info = data["exchange_rate"]
        base = rate_info.get("base")
        target = rate_info.get("target")
        agent_rate = rate_info.get("rate")
        
        if base and target and agent_rate is not None:
            try:
                # Call ground truth
                gt_exchange = fetch_exchange_rate(base, target)
                gt_rate = gt_exchange["exchange_rate"]["rate"]
                
                results["exchange_match"] = abs(agent_rate - gt_rate) < 0.01
                results["exchange_details"] = {"agent": agent_rate, "ground_truth": gt_rate}
            except Exception as e:
                results["exchange_evaluation_error"] = str(e)

    if not results:
        return {
            "status": "no_data",
            "message": "No valid weather or exchange rate data found in the agent output to evaluate.",
            "data_received": str(data)[:200]
        }

    return results

  